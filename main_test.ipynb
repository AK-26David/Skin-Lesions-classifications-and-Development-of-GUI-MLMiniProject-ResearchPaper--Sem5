{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Features ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_glcm_features(image):\n",
    "    # Convert to grayscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Compute the GLCM (Gray Level Co-occurrence Matrix)\n",
    "    glcm = greycomatrix(gray_image, distances=[1], angles=[0], symmetric=True, normed=True)\n",
    "    \n",
    "    # Extract texture properties: Contrast, Correlation, Energy, and Homogeneity\n",
    "    contrast = greycoprops(glcm, 'contrast')[0, 0]\n",
    "    correlation = greycoprops(glcm, 'correlation')[0, 0]\n",
    "    energy = greycoprops(glcm, 'energy')[0, 0]\n",
    "    homogeneity = greycoprops(glcm, 'homogeneity')[0, 0]\n",
    "    \n",
    "    return [contrast, correlation, energy, homogeneity]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hog_features(image):\n",
    "    # Convert to grayscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Compute HOG features\n",
    "    hog_features, hog_image = hog(gray_image, \n",
    "                                  pixels_per_cell=(16, 16), \n",
    "                                  cells_per_block=(2, 2), \n",
    "                                  block_norm='L2-Hys', \n",
    "                                  visualize=True, \n",
    "                                  multichannel=False)\n",
    "    \n",
    "    return hog_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # Loop through subfolders, where each folder represents a class (e.g., Benign or Malignant)\n",
    "    for label_folder in os.listdir(folder):\n",
    "        label_path = os.path.join(folder, label_folder)\n",
    "        if os.path.isdir(label_path):\n",
    "            for filename in os.listdir(label_path):\n",
    "                img_path = os.path.join(label_path, filename)\n",
    "                img = cv2.imread(img_path)  # Read the image\n",
    "                if img is not None:\n",
    "                    images.append(img)\n",
    "                    labels.append(label_folder)  # The folder name becomes the class label\n",
    "    return images, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(X_images):\n",
    "    X_features = []\n",
    "    \n",
    "    for img in X_images:\n",
    "        # Extract GLCM (texture) features\n",
    "        glcm_features = extract_glcm_features(img)\n",
    "        \n",
    "        # Extract HOG (shape) features\n",
    "        hog_features = extract_hog_features(img)\n",
    "        \n",
    "        # Combine both sets of features\n",
    "        combined_features = np.concatenate((glcm_features, hog_features))\n",
    "        \n",
    "        # Add the combined features to the feature list\n",
    "        X_features.append(combined_features)\n",
    "        \n",
    "    return np.array(X_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m X_features \u001b[38;5;241m=\u001b[39m prepare_data(X_images)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Split dataset into training and test sets (80% train, 20% test)\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Scale the feature data\u001b[39;00m\n\u001b[1;32m     11\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n",
      "File \u001b[0;32m~/Documents/Skin-Lesions-classifications-and-Development-of-GUI/.venv/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/Skin-Lesions-classifications-and-Development-of-GUI/.venv/lib/python3.9/site-packages/sklearn/model_selection/_split.py:2785\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2782\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[1;32m   2784\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m-> 2785\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\n\u001b[1;32m   2787\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m   2790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/Skin-Lesions-classifications-and-Development-of-GUI/.venv/lib/python3.9/site-packages/sklearn/model_selection/_split.py:2415\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2412\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[1;32m   2414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2415\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2416\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2417\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2418\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[1;32m   2419\u001b[0m     )\n\u001b[1;32m   2421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "# Load dataset (assuming images are stored in 'data' folder)\n",
    "X_images, y_labels = load_images_from_folder('ISIC-images')\n",
    "\n",
    "# Prepare feature set by extracting features from the images\n",
    "X_features = prepare_data(X_images)\n",
    "\n",
    "# Split dataset into training and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, y_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the feature data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@400.466] global loadsave.cpp:241 findDecoder imread_('path_to_your_image.jpg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 116\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# Usage\u001b[39;00m\n\u001b[1;32m    115\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath_to_your_image.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 116\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_lesion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features from the image.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 104\u001b[0m, in \u001b[0;36manalyze_lesion\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21manalyze_lesion\u001b[39m(image_path):\n\u001b[0;32m--> 104\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     color_features \u001b[38;5;241m=\u001b[39m extract_color_features(img)\n\u001b[1;32m    107\u001b[0m     texture_features \u001b[38;5;241m=\u001b[39m extract_texture_features(img)\n",
      "Cell \u001b[0;32mIn[9], line 10\u001b[0m, in \u001b[0;36mpreprocess_image\u001b[0;34m(image_path, target_size)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_image\u001b[39m(image_path, target_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m300\u001b[39m, \u001b[38;5;241m300\u001b[39m)):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Load image\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(image_path)\n\u001b[0;32m---> 10\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2RGB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Resize\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(img, target_size)\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.10.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.feature import local_binary_pattern\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "from scipy.stats import skew\n",
    "\n",
    "def preprocess_image(image_path, target_size=(300, 300)):\n",
    "    # Load image\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Resize\n",
    "    img = cv2.resize(img, target_size)\n",
    "    \n",
    "    # Extract the circular region of interest\n",
    "    mask = np.zeros(img.shape[:2], dtype=np.uint8)\n",
    "    cv2.circle(mask, (img.shape[1]//2, img.shape[0]//2), min(img.shape[0], img.shape[1])//2 - 10, (255), -1)\n",
    "    img = cv2.bitwise_and(img, img, mask=mask)\n",
    "    \n",
    "    # Enhance contrast\n",
    "    lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    cl = clahe.apply(l)\n",
    "    enhanced_lab = cv2.merge((cl,a,b))\n",
    "    img = cv2.cvtColor(enhanced_lab, cv2.COLOR_LAB2RGB)\n",
    "    \n",
    "    # Normalize\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "    \n",
    "    return img\n",
    "\n",
    "def extract_color_features(img):\n",
    "    features = []\n",
    "    \n",
    "    # Color moments (mean, std, skewness) for each channel\n",
    "    for i in range(3):\n",
    "        channel = img[:,:,i]\n",
    "        features.extend([channel[channel != 0].mean(), channel[channel != 0].std(), skew(channel[channel != 0].ravel())])\n",
    "    \n",
    "    # Color histograms\n",
    "    for i in range(3):\n",
    "        hist = cv2.calcHist([img], [i], None, [32], [0, 1])\n",
    "        features.extend(hist.flatten())\n",
    "    \n",
    "    return features\n",
    "\n",
    "def extract_texture_features(img):\n",
    "    features = []\n",
    "    \n",
    "    gray = cv2.cvtColor((img * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Local Binary Patterns\n",
    "    radius = 3\n",
    "    n_points = 8 * radius\n",
    "    lbp = local_binary_pattern(gray, n_points, radius, method='uniform')\n",
    "    hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, n_points + 3), range=(0, n_points + 2))\n",
    "    features.extend(hist)\n",
    "    \n",
    "    # Haralick texture features\n",
    "    glcm = graycomatrix(gray, [5], [0], 256, symmetric=True, normed=True)\n",
    "    props = ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation']\n",
    "    for prop in props:\n",
    "        features.extend(graycoprops(glcm, prop).flatten())\n",
    "    \n",
    "    return features\n",
    "\n",
    "def extract_shape_features(img):\n",
    "    features = []\n",
    "    \n",
    "    gray = cv2.cvtColor((img * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    if contours:\n",
    "        cnt = max(contours, key=cv2.contourArea)\n",
    "        \n",
    "        # Area and Perimeter\n",
    "        area = cv2.contourArea(cnt)\n",
    "        perimeter = cv2.arcLength(cnt, True)\n",
    "        features.extend([area, perimeter])\n",
    "        \n",
    "        # Circularity\n",
    "        circularity = 4 * np.pi * area / (perimeter ** 2)\n",
    "        features.append(circularity)\n",
    "        \n",
    "        # Asymmetry\n",
    "        (x, y), (MA, ma), angle = cv2.fitEllipse(cnt)\n",
    "        asymmetry = MA / ma\n",
    "        features.append(asymmetry)\n",
    "        \n",
    "        # Border irregularity\n",
    "        hull = cv2.convexHull(cnt)\n",
    "        hull_area = cv2.contourArea(hull)\n",
    "        solidity = float(area) / hull_area\n",
    "        features.append(solidity)\n",
    "    else:\n",
    "        features.extend([0] * 5)  # Placeholder if no contour found\n",
    "    \n",
    "    return features\n",
    "\n",
    "def analyze_lesion(image_path):\n",
    "    img = preprocess_image(image_path)\n",
    "    \n",
    "    color_features = extract_color_features(img)\n",
    "    texture_features = extract_texture_features(img)\n",
    "    shape_features = extract_shape_features(img)\n",
    "    \n",
    "    all_features = np.concatenate([color_features, texture_features, shape_features])\n",
    "    \n",
    "    return all_features\n",
    "\n",
    "# Usage\n",
    "image_path = \"path_to_your_image.jpg\"\n",
    "features = analyze_lesion(image_path)\n",
    "print(f\"Extracted {len(features)} features from the image.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 25431/25431 [1:22:39<00:00,  5.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 25431 images. Results saved to 'skin_lesion_features.csv'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.feature import local_binary_pattern\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "from scipy.stats import skew\n",
    "from tqdm import tqdm\n",
    "\n",
    "def preprocess_image(img, target_size=(300, 300)):\n",
    "    # Resize\n",
    "    img = cv2.resize(img, target_size)\n",
    "    \n",
    "    # Extract the circular region of interest\n",
    "    mask = np.zeros(img.shape[:2], dtype=np.uint8)\n",
    "    cv2.circle(mask, (img.shape[1]//2, img.shape[0]//2), min(img.shape[0], img.shape[1])//2 - 10, (255), -1)\n",
    "    img = cv2.bitwise_and(img, img, mask=mask)\n",
    "    \n",
    "    # Enhance contrast\n",
    "    lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    cl = clahe.apply(l)\n",
    "    enhanced_lab = cv2.merge((cl,a,b))\n",
    "    img = cv2.cvtColor(enhanced_lab, cv2.COLOR_LAB2RGB)\n",
    "    \n",
    "    # Normalize\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "    \n",
    "    return img\n",
    "\n",
    "def extract_color_features(img):\n",
    "    features = []\n",
    "    \n",
    "    # Color moments (mean, std, skewness) for each channel\n",
    "    for i in range(3):\n",
    "        channel = img[:,:,i]\n",
    "        features.extend([channel[channel != 0].mean(), channel[channel != 0].std(), skew(channel[channel != 0].ravel())])\n",
    "    \n",
    "    # Color histograms\n",
    "    for i in range(3):\n",
    "        hist = cv2.calcHist([img], [i], None, [32], [0, 1])\n",
    "        features.extend(hist.flatten())\n",
    "    \n",
    "    return features\n",
    "\n",
    "def gabor_features(img, frequencies=[0.1, 0.2, 0.3], orientations=[0, 45, 90, 135]):\n",
    "    features = []\n",
    "    for theta in orientations:\n",
    "        for frequency in frequencies:\n",
    "            # Create Gabor filter\n",
    "            kernel = cv2.getGaborKernel((21, 21), 8.0, np.radians(theta), frequency, 0.5, 0, ktype=cv2.CV_32F)\n",
    "            filtered = cv2.filter2D(img, cv2.CV_8UC3, kernel)\n",
    "\n",
    "            # Calculate mean and standard deviation of the filtered image\n",
    "            features.append(filtered.mean())\n",
    "            features.append(filtered.std())\n",
    "    \n",
    "    return features\n",
    "\n",
    "def extract_texture_features(img):\n",
    "    features = []\n",
    "    \n",
    "    gray = cv2.cvtColor((img * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Local Binary Patterns\n",
    "    radius = 3\n",
    "    n_points = 8 * radius\n",
    "    lbp = local_binary_pattern(gray, n_points, radius, method='uniform')\n",
    "    hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, n_points + 3), range=(0, n_points + 2))\n",
    "    features.extend(hist)\n",
    "    \n",
    "    # Haralick texture features\n",
    "    glcm = graycomatrix(gray, [5], [0], 256, symmetric=True, normed=True)\n",
    "    props = ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation']\n",
    "    for prop in props:\n",
    "        features.extend(graycoprops(glcm, prop).flatten())\n",
    "    \n",
    "    # Gabor filter features\n",
    "    features.extend(gabor_features(gray))  # Add Gabor features\n",
    "    \n",
    "    return features\n",
    "\n",
    "def extract_shape_features(img):\n",
    "    features = []\n",
    "    \n",
    "    gray = cv2.cvtColor((img * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    if contours:\n",
    "        cnt = max(contours, key=cv2.contourArea)\n",
    "        \n",
    "        # Area and Perimeter\n",
    "        area = cv2.contourArea(cnt)\n",
    "        perimeter = cv2.arcLength(cnt, True)\n",
    "        features.extend([area, perimeter])\n",
    "        \n",
    "        # Circularity\n",
    "        circularity = 4 * np.pi * area / (perimeter ** 2)\n",
    "        features.append(circularity)\n",
    "        \n",
    "        # Asymmetry\n",
    "        (x, y), (MA, ma), angle = cv2.fitEllipse(cnt)\n",
    "        asymmetry = MA / ma\n",
    "        features.append(asymmetry)\n",
    "        \n",
    "        # Border irregularity\n",
    "        hull = cv2.convexHull(cnt)\n",
    "        hull_area = cv2.contourArea(hull)\n",
    "        solidity = float(area) / hull_area\n",
    "        features.append(solidity)\n",
    "    else:\n",
    "        features.extend([0] * 5)  # Placeholder if no contour found\n",
    "    \n",
    "    return features\n",
    "\n",
    "def analyze_lesion(img):\n",
    "    img = preprocess_image(img)\n",
    "    \n",
    "    color_features = extract_color_features(img)\n",
    "    texture_features = extract_texture_features(img)\n",
    "    shape_features = extract_shape_features(img)\n",
    "    \n",
    "    all_features = np.concatenate([color_features, texture_features, shape_features])\n",
    "    \n",
    "    return all_features\n",
    "\n",
    "def process_image_folder(folder_path):\n",
    "    all_features = []\n",
    "    image_names = []\n",
    "    \n",
    "    # Get all image files\n",
    "    image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    \n",
    "    for image_file in tqdm(image_files, desc=\"Processing images\"):\n",
    "        image_path = os.path.join(folder_path, image_file)\n",
    "        img = cv2.imread(image_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        features = analyze_lesion(img)\n",
    "        all_features.append(features)\n",
    "        image_names.append(image_file)\n",
    "    \n",
    "    # Create a DataFrame with the features\n",
    "    feature_names = [f'feature_{i}' for i in range(len(all_features[0]))]\n",
    "    df = pd.DataFrame(all_features, columns=feature_names)\n",
    "    df['image_name'] = image_names\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Usage\n",
    "folder_path = \"ISIC-images\"\n",
    "results_df = process_image_folder(folder_path)\n",
    "\n",
    "# Save the results\n",
    "results_df.to_csv(\"skin_lesion_features.csv\", index=False)\n",
    "print(f\"Processed {len(results_df)} images. Results saved to 'skin_lesion_features.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 100/100 [00:02<00:00, 36.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 images. Results saved to 'skin_lesion_features.csv'\n",
      "Total number of features: 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.feature import local_binary_pattern\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "from scipy.stats import skew\n",
    "from tqdm import tqdm\n",
    "\n",
    "def preprocess_image(img, target_size=(300, 300)):\n",
    "    # Resize\n",
    "    img = cv2.resize(img, target_size)\n",
    "    \n",
    "    # Extract the circular region of interest\n",
    "    mask = np.zeros(img.shape[:2], dtype=np.uint8)\n",
    "    cv2.circle(mask, (img.shape[1]//2, img.shape[0]//2), min(img.shape[0], img.shape[1])//2 - 10, (255), -1)\n",
    "    img = cv2.bitwise_and(img, img, mask=mask)\n",
    "    \n",
    "    # Enhance contrast\n",
    "    lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    cl = clahe.apply(l)\n",
    "    enhanced_lab = cv2.merge((cl,a,b))\n",
    "    img = cv2.cvtColor(enhanced_lab, cv2.COLOR_LAB2RGB)\n",
    "    \n",
    "    # Normalize\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "    \n",
    "    return img\n",
    "\n",
    "def extract_color_features(img):\n",
    "    features = {}\n",
    "    channels = ['R', 'G', 'B']\n",
    "    \n",
    "    # Color moments (mean, std, skewness) for each channel\n",
    "    for i, channel in enumerate(channels):\n",
    "        channel_data = img[:,:,i]\n",
    "        features[f'color_mean_{channel}'] = channel_data[channel_data != 0].mean()\n",
    "        features[f'color_std_{channel}'] = channel_data[channel_data != 0].std()\n",
    "        features[f'color_skew_{channel}'] = skew(channel_data[channel_data != 0].ravel())\n",
    "    \n",
    "    # Color histograms\n",
    "    for i, channel in enumerate(channels):\n",
    "        hist = cv2.calcHist([img], [i], None, [32], [0, 1])\n",
    "        for j, value in enumerate(hist.flatten()):\n",
    "            features[f'color_hist_{channel}_{j}'] = value\n",
    "    \n",
    "    return features\n",
    "\n",
    "def extract_texture_features(img):\n",
    "    features = {}\n",
    "    \n",
    "    gray = cv2.cvtColor((img * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Local Binary Patterns\n",
    "    radius = 3\n",
    "    n_points = 8 * radius\n",
    "    lbp = local_binary_pattern(gray, n_points, radius, method='uniform')\n",
    "    hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, n_points + 3), range=(0, n_points + 2))\n",
    "    for i, value in enumerate(hist):\n",
    "        features[f'lbp_{i}'] = value\n",
    "    \n",
    "    # Haralick texture features\n",
    "    glcm = graycomatrix(gray, [5], [0], 256, symmetric=True, normed=True)\n",
    "    props = ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation']\n",
    "    for prop in props:\n",
    "        value = graycoprops(glcm, prop)[0, 0]\n",
    "        features[f'haralick_{prop}'] = value\n",
    "    \n",
    "    return features\n",
    "\n",
    "def extract_shape_features(img):\n",
    "    features = {}\n",
    "    \n",
    "    gray = cv2.cvtColor((img * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    if contours:\n",
    "        cnt = max(contours, key=cv2.contourArea)\n",
    "        \n",
    "        # Area and Perimeter\n",
    "        area = cv2.contourArea(cnt)\n",
    "        perimeter = cv2.arcLength(cnt, True)\n",
    "        features['shape_area'] = area\n",
    "        features['shape_perimeter'] = perimeter\n",
    "        \n",
    "        # Circularity\n",
    "        circularity = 4 * np.pi * area / (perimeter ** 2)\n",
    "        features['shape_circularity'] = circularity\n",
    "        \n",
    "        # Asymmetry\n",
    "        (x, y), (MA, ma), angle = cv2.fitEllipse(cnt)\n",
    "        asymmetry = MA / ma\n",
    "        features['shape_asymmetry'] = asymmetry\n",
    "        \n",
    "        # Border irregularity\n",
    "        hull = cv2.convexHull(cnt)\n",
    "        hull_area = cv2.contourArea(hull)\n",
    "        solidity = float(area) / hull_area\n",
    "        features['shape_border_irregularity'] = solidity\n",
    "    else:\n",
    "        for feature in ['shape_area', 'shape_perimeter', 'shape_circularity', 'shape_asymmetry', 'shape_border_irregularity']:\n",
    "            features[feature] = 0\n",
    "    \n",
    "    return features\n",
    "\n",
    "def analyze_lesion(img):\n",
    "    img = preprocess_image(img)\n",
    "    \n",
    "    features = {}\n",
    "    features.update(extract_color_features(img))\n",
    "    features.update(extract_texture_features(img))\n",
    "    features.update(extract_shape_features(img))\n",
    "    \n",
    "    return features\n",
    "\n",
    "def process_image_folder(folder_path):\n",
    "    all_features = []\n",
    "    image_names = []\n",
    "    \n",
    "    # Get all image files\n",
    "    image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    \n",
    "    for image_file in tqdm(image_files, desc=\"Processing images\"):\n",
    "        image_path = os.path.join(folder_path, image_file)\n",
    "        img = cv2.imread(image_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        features = analyze_lesion(img)\n",
    "        all_features.append(features)\n",
    "        image_names.append(image_file)\n",
    "    \n",
    "    # Create a DataFrame with the features\n",
    "    df = pd.DataFrame(all_features)\n",
    "    df['image_name'] = image_names\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Usage\n",
    "folder_path = \"ISIC-images\"\n",
    "results_df = process_image_folder(folder_path)\n",
    "\n",
    "# Save the results\n",
    "results_df.to_csv(\"skin_lesion_features.csv\", index=False)\n",
    "print(f\"Processed {len(results_df)} images. Results saved to 'skin_lesion_features.csv'\")\n",
    "print(f\"Total number of features: {len(results_df.columns) - 1}\")  # -1 for the image_name column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Step 1: Load the newly generated data after improving feature extraction\n",
    "df = pd.read_csv(\"skin_lesion_features.csv\")\n",
    "\n",
    "# Step 2: Assume we have a target variable called 'label'\n",
    "# You should replace this with the actual labels after loading the proper dataset\n",
    "df['label'] = np.random.choice(['benign', 'malignant'], size=len(df))\n",
    "\n",
    "# Step 3: Separate features and target\n",
    "X = df.drop(['image_name', 'label'], axis=1)\n",
    "y = df['label']\n",
    "\n",
    "# Step 4: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 6: SVM Model - Perform Grid Search for hyperparameter tuning\n",
    "svm_param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['rbf', 'poly', 'linear'],  # Added 'linear' kernel for feature importance\n",
    "    'gamma': ['scale', 'auto', 0.1, 1]\n",
    "}\n",
    "\n",
    "svm_grid = GridSearchCV(SVC(random_state=42), svm_param_grid, cv=5, n_jobs=-1)\n",
    "svm_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 7: Evaluate SVM Model\n",
    "print(\"SVM Best Parameters:\", svm_grid.best_params_)\n",
    "print(\"SVM Best Score:\", svm_grid.best_score_)\n",
    "\n",
    "svm_pred = svm_grid.predict(X_test_scaled)\n",
    "print(\"\\nSVM Classification Report:\")\n",
    "print(classification_report(y_test, svm_pred))\n",
    "\n",
    "# Step 8: MLP Model - Perform Grid Search for hyperparameter tuning\n",
    "mlp_param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate': ['constant', 'adaptive']\n",
    "}\n",
    "\n",
    "mlp_grid = GridSearchCV(MLPClassifier(random_state=42, max_iter=1000), mlp_param_grid, cv=5, n_jobs=-1)\n",
    "mlp_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 9: Evaluate MLP Model\n",
    "print(\"\\nMLP Best Parameters:\", mlp_grid.best_params_)\n",
    "print(\"MLP Best Score:\", mlp_grid.best_score_)\n",
    "\n",
    "mlp_pred = mlp_grid.predict(X_test_scaled)\n",
    "print(\"\\nMLP Classification Report:\")\n",
    "print(classification_report(y_test, mlp_pred))\n",
    "\n",
    "# Step 10: Plot Confusion Matrices for both SVM and MLP\n",
    "def plot_confusion_matrix(y_true, y_pred, title):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(y_test, svm_pred, \"SVM Confusion Matrix\")\n",
    "plot_confusion_matrix(y_test, mlp_pred, \"MLP Confusion Matrix\")\n",
    "\n",
    "# Step 11: Feature Importance (for SVM with linear kernel)\n",
    "if svm_grid.best_estimator_.kernel == 'linear':\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': abs(svm_grid.best_estimator_.coef_[0])\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='importance', y='feature', data=feature_importance.head(20))\n",
    "    plt.title('Top 20 Important Features (SVM)')\n",
    "    plt.show()\n",
    "\n",
    "# Step 12: Predictions on New Data\n",
    "new_data = X_test_scaled[:5]  # Example: First 5 samples of the test set\n",
    "svm_new_pred = svm_grid.predict(new_data)\n",
    "mlp_new_pred = mlp_grid.predict(new_data)\n",
    "\n",
    "print(\"\\nSVM Predictions on new data:\", svm_new_pred)\n",
    "print(\"MLP Predictions on new data:\", mlp_new_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading malignant images: 100%|██████████| 1197/1197 [00:06<00:00, 172.47it/s]\n",
      "Loading benign images: 100%|██████████| 1440/1440 [00:04<00:00, 347.13it/s]\n",
      "Loading malignant images: 100%|██████████| 300/300 [00:01<00:00, 238.16it/s]\n",
      "Loading benign images: 100%|██████████| 360/360 [00:02<00:00, 136.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# FROM HERE ON IS MY PART THAT IS WORKING .....\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_dataset(folder_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for label in ['malignant', 'benign']:\n",
    "        folder = os.path.join(folder_path, label)\n",
    "        for img_name in tqdm(os.listdir(folder), desc=f\"Loading {label} images\"):\n",
    "            img_path = os.path.join(folder, img_name)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:\n",
    "                if len(img.shape) == 2:  # Grayscale image (2D)\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)  # Convert grayscale to RGB\n",
    "                else:\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB from BGR\n",
    "                data.append(img)\n",
    "                labels.append(label)\n",
    "    \n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "\n",
    "# Load train and test sets\n",
    "train_data, train_labels = load_dataset(\"train\")\n",
    "test_data, test_labels = load_dataset(\"test\")\n",
    "\n",
    "# Convert labels to binary format (0 for benign, 1 for malignant)\n",
    "train_labels = np.where(train_labels == 'benign', 0, 1)\n",
    "test_labels = np.where(test_labels == 'benign', 0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 2637/2637 [02:48<00:00, 15.68it/s]\n",
      "Processing images: 100%|██████████| 660/660 [00:40<00:00, 16.29it/s]\n"
     ]
    }
   ],
   "source": [
    "def extract_shape_features(img):\n",
    "    features = []\n",
    "\n",
    "    gray = cv2.cvtColor((img * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if contours:\n",
    "        cnt = max(contours, key=cv2.contourArea)\n",
    "\n",
    "        # Area and Perimeter\n",
    "        area = cv2.contourArea(cnt)\n",
    "        perimeter = cv2.arcLength(cnt, True)\n",
    "        features.extend([area, perimeter])\n",
    "\n",
    "        # Circularity\n",
    "        circularity = 4 * np.pi * area / (perimeter ** 2)\n",
    "        features.append(circularity)\n",
    "\n",
    "        # Check if contour has at least 5 points before fitting an ellipse\n",
    "        if len(cnt) >= 5:\n",
    "            (x, y), (MA, ma), angle = cv2.fitEllipse(cnt)\n",
    "            asymmetry = MA / ma\n",
    "            features.append(asymmetry)\n",
    "        else:\n",
    "            # Add a placeholder value if ellipse fitting is not possible\n",
    "            features.append(0)\n",
    "\n",
    "        # Border irregularity\n",
    "        hull = cv2.convexHull(cnt)\n",
    "        hull_area = cv2.contourArea(hull)\n",
    "        solidity = float(area) / hull_area\n",
    "        features.append(solidity)\n",
    "    else:\n",
    "        # Placeholder if no contour is found\n",
    "        features.extend([0] * 5)\n",
    "\n",
    "    return features\n",
    "\n",
    "def extract_texture_features(img):\n",
    "    features = {}\n",
    "    \n",
    "    gray = cv2.cvtColor((img * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Local Binary Patterns\n",
    "    radius = 3\n",
    "    n_points = 8 * radius\n",
    "    lbp = local_binary_pattern(gray, n_points, radius, method='uniform')\n",
    "    hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, n_points + 3), range=(0, n_points + 2))\n",
    "    for i, value in enumerate(hist):\n",
    "        features[f'lbp_{i}'] = value\n",
    "    \n",
    "    # Haralick texture features\n",
    "    glcm = graycomatrix(gray, [5], [0], 256, symmetric=True, normed=True)\n",
    "    props = ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation']\n",
    "    for prop in props:\n",
    "        value = graycoprops(glcm, prop)[0, 0]\n",
    "        features[f'haralick_{prop}'] = value\n",
    "    \n",
    "    return features\n",
    "\n",
    "def extract_color_features(img):\n",
    "    features = []\n",
    "    \n",
    "    # Color moments (mean, std, skewness) for each channel\n",
    "    for i in range(3):\n",
    "        channel = img[:,:,i]\n",
    "        features.extend([channel[channel != 0].mean(), channel[channel != 0].std(), skew(channel[channel != 0].ravel())])\n",
    "    \n",
    "    # Color histograms\n",
    "    for i in range(3):\n",
    "        hist = cv2.calcHist([img], [i], None, [32], [0, 1])\n",
    "        features.extend(hist.flatten())\n",
    "    \n",
    "    return features\n",
    "\n",
    "def preprocess_image(img):    \n",
    "    # Check if the image is RGB (3 channels)\n",
    "    if len(img.shape) == 3 and img.shape[2] == 3:  # RGB image\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)  # Convert to grayscale\n",
    "    else:  # Image is already grayscale (2D)\n",
    "        gray_img = img\n",
    "    \n",
    "    # Apply CLAHE to the grayscale image\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    clahe_img = clahe.apply(gray_img)\n",
    "    \n",
    "    return clahe_img\n",
    "\n",
    "def analyze_lesion(img):\n",
    "    # Check if the image is grayscale (2D) and convert it to RGB (3D) if necessary\n",
    "    if len(img.shape) == 2:  # If the image is grayscale\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)  # Convert grayscale to RGB\n",
    "\n",
    "    color_features = extract_color_features(img)\n",
    "    texture_features = extract_texture_features(img)\n",
    "    shape_features = extract_shape_features(img)\n",
    "\n",
    "    return np.concatenate([color_features, texture_features, shape_features])\n",
    "\n",
    "\n",
    "def process_dataset(data, labels):\n",
    "    features = []\n",
    "    for img in tqdm(data, desc=\"Processing images\"):\n",
    "        processed_img = preprocess_image(img)  # Preprocess the image\n",
    "        img_features = analyze_lesion(processed_img)  # Extract color, texture, shape features\n",
    "        features.append(img_features)\n",
    "    \n",
    "    features = np.array(features)\n",
    "    return features, labels\n",
    "\n",
    "# Process train and test sets\n",
    "X_train, y_train = process_dataset(train_data, train_labels)\n",
    "X_test, y_test = process_dataset(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Best Parameters: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "SVM Best Score: 0.8028082053935943\n",
      "\n",
      "SVM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.85      0.85       360\n",
      "           1       0.82      0.80      0.81       300\n",
      "\n",
      "    accuracy                           0.83       660\n",
      "   macro avg       0.83      0.83      0.83       660\n",
      "weighted avg       0.83      0.83      0.83       660\n",
      "\n",
      "\n",
      "MLP Best Parameters: {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant'}\n",
      "MLP Best Score: 0.8035672186763267\n",
      "\n",
      "MLP Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.82      0.83       360\n",
      "           1       0.79      0.80      0.79       300\n",
      "\n",
      "    accuracy                           0.81       660\n",
      "   macro avg       0.81      0.81      0.81       660\n",
      "weighted avg       0.81      0.81      0.81       660\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Split and scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# SVM Model\n",
    "svm_param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['rbf', 'poly'],\n",
    "    'gamma': ['scale', 'auto', 0.1, 1]\n",
    "}\n",
    "\n",
    "svm_grid = GridSearchCV(SVC(random_state=42), svm_param_grid, cv=5, n_jobs=-1)\n",
    "svm_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"SVM Best Parameters:\", svm_grid.best_params_)\n",
    "print(\"SVM Best Score:\", svm_grid.best_score_)\n",
    "\n",
    "svm_pred = svm_grid.predict(X_test_scaled)\n",
    "print(\"\\nSVM Classification Report:\")\n",
    "print(classification_report(y_test, svm_pred))\n",
    "\n",
    "# MLP Model\n",
    "mlp_param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate': ['constant', 'adaptive']\n",
    "}\n",
    "\n",
    "mlp_grid = GridSearchCV(MLPClassifier(random_state=42, max_iter=1000), mlp_param_grid, cv=5, n_jobs=-1)\n",
    "mlp_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"\\nMLP Best Parameters:\", mlp_grid.best_params_)\n",
    "print(\"MLP Best Score:\", mlp_grid.best_score_)\n",
    "\n",
    "mlp_pred = mlp_grid.predict(X_test_scaled)\n",
    "print(\"\\nMLP Classification Report:\")\n",
    "print(classification_report(y_test, mlp_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arnav Karnik ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage import feature, exposure, filters, segmentation, measure\n",
    "from scipy.stats import skew, kurtosis\n",
    "from skimage.feature import graycomatrix, graycoprops, local_binary_pattern, hog\n",
    "from skimage.filters import gabor, frangi, sato\n",
    "from scipy.spatial import distance\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "class SkinLesionAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.results = {}\n",
    "        \n",
    "    def preprocess_image(self, image):\n",
    "        \"\"\"\n",
    "        Enhanced preprocessing pipeline with advanced CV techniques\n",
    "        \"\"\"\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        original = image.copy()\n",
    "        \n",
    "        # Multi-scale segmentation\n",
    "        mask = self.create_lesion_mask_multiscale(image)\n",
    "        \n",
    "        # Enhanced preprocessing pipeline\n",
    "        preprocessed = self.apply_preprocessing_pipeline(image, mask)\n",
    "        \n",
    "        self.results['original'] = original\n",
    "        self.results['mask'] = mask\n",
    "        self.results['preprocessed'] = preprocessed\n",
    "        \n",
    "        return preprocessed\n",
    "    \n",
    "    def create_lesion_mask_multiscale(self, image):\n",
    "        \"\"\"\n",
    "        Create binary mask using multi-scale analysis\n",
    "        \"\"\"\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # Multi-scale Gaussian pyramid\n",
    "        masks = []\n",
    "        current = gray.copy()\n",
    "        for _ in range(3):  # Analysis at 3 scales\n",
    "            # Apply SLIC superpixels\n",
    "            segments = segmentation.slic(current, n_segments=100, compactness=10)\n",
    "            \n",
    "            # Graph-based segmentation\n",
    "            graph_mask = segmentation.felzenszwalb(current, scale=100, sigma=0.5, min_size=50)\n",
    "            \n",
    "            # Watershed segmentation\n",
    "            gradient = filters.sobel(current)\n",
    "            markers = measure.label(gradient < gradient.mean())\n",
    "            watershed_mask = segmentation.watershed(gradient, markers)\n",
    "            \n",
    "            # Combine segmentations\n",
    "            combined = (segments > 0) & (graph_mask > 0) & (watershed_mask > 0)\n",
    "            masks.append(cv2.resize(combined.astype(float), (gray.shape[1], gray.shape[0])))\n",
    "            \n",
    "            # Downsample for next scale\n",
    "            current = cv2.pyrDown(current)\n",
    "        \n",
    "        # Combine masks from different scales\n",
    "        final_mask = np.mean(masks, axis=0) > 0.5\n",
    "        \n",
    "        # Post-process mask\n",
    "        kernel = np.ones((5,5), np.uint8)\n",
    "        final_mask = cv2.morphologyEx(final_mask.astype(np.uint8), cv2.MORPH_CLOSE, kernel)\n",
    "        final_mask = cv2.morphologyEx(final_mask, cv2.MORPH_OPEN, kernel)\n",
    "        \n",
    "        return final_mask\n",
    "    \n",
    "    def apply_preprocessing_pipeline(self, image, mask):\n",
    "        \"\"\"\n",
    "        Enhanced preprocessing pipeline with advanced filtering\n",
    "        \"\"\"\n",
    "        # Non-local means denoising\n",
    "        processed = cv2.fastNlMeansDenoisingColored(image, None, 10, 10, 7, 21)\n",
    "        \n",
    "        # Hair removal with enhanced detection\n",
    "        processed = self.remove_hair_advanced(processed)\n",
    "        \n",
    "        # Illumination correction using DoG\n",
    "        processed = self.correct_illumination(processed)\n",
    "        \n",
    "        # Advanced color normalization\n",
    "        processed = self.normalize_color_advanced(processed)\n",
    "        \n",
    "        # Multi-scale contrast enhancement\n",
    "        processed = self.enhance_contrast_multiscale(processed)\n",
    "        \n",
    "        # Apply mask\n",
    "        processed = processed * np.stack([mask]*3, axis=-1)\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    def remove_hair_advanced(self, image):\n",
    "        \"\"\"\n",
    "        Advanced hair removal using multi-scale line detection\n",
    "        \"\"\"\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # Multi-scale line detection\n",
    "        hair_mask = np.zeros_like(gray)\n",
    "        for sigma in [1, 2, 3]:\n",
    "            # Frangi filter for line detection\n",
    "            line_mask = frangi(gray, sigmas=range(1, sigma + 1), black_ridges=True)\n",
    "            hair_mask = np.maximum(hair_mask, line_mask)\n",
    "        \n",
    "        # Threshold and clean up\n",
    "        hair_mask = (hair_mask > 0.1).astype(np.uint8)\n",
    "        \n",
    "        # Dilate to ensure complete hair coverage\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5,5))\n",
    "        hair_mask = cv2.dilate(hair_mask, kernel, iterations=1)\n",
    "        \n",
    "        # Inpaint using Navier-Stokes method\n",
    "        hair_removed = cv2.inpaint(image, hair_mask, 5, cv2.INPAINT_NS)\n",
    "        \n",
    "        return hair_removed\n",
    "    \n",
    "    def correct_illumination(self, image):\n",
    "        \"\"\"\n",
    "        Correct uneven illumination using Difference of Gaussians\n",
    "        \"\"\"\n",
    "        # Convert to LAB\n",
    "        lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n",
    "        l, a, b = cv2.split(lab)\n",
    "        \n",
    "        # Apply DoG\n",
    "        gaussian1 = cv2.GaussianBlur(l, (0,0), 15)\n",
    "        gaussian2 = cv2.GaussianBlur(l, (0,0), 2)\n",
    "        dog = gaussian1 - gaussian2\n",
    "        \n",
    "        # Normalize and enhance L channel\n",
    "        l = exposure.rescale_intensity(l - dog)\n",
    "        \n",
    "        # Merge channels\n",
    "        corrected = cv2.merge([l, a, b])\n",
    "        return cv2.cvtColor(corrected, cv2.COLOR_LAB2RGB)\n",
    "    \n",
    "    def normalize_color_advanced(self, image):\n",
    "        \"\"\"\n",
    "        Advanced color normalization using color transfer and statistics\n",
    "        \"\"\"\n",
    "        # Convert to LAB\n",
    "        lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n",
    "        \n",
    "        # Color clustering\n",
    "        pixels = lab.reshape(-1, 3)\n",
    "        kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "        labels = kmeans.fit_predict(pixels)\n",
    "        \n",
    "        # Find dominant colors\n",
    "        dominant_colors = kmeans.cluster_centers_\n",
    "        \n",
    "        # Normalize based on dominant colors\n",
    "        normalized = np.zeros_like(lab, dtype=np.float32)\n",
    "        for i in range(len(dominant_colors)):\n",
    "            mask = (labels == i).reshape(lab.shape[:2])\n",
    "            color_mean = dominant_colors[i]\n",
    "            color_std = np.std(pixels[labels == i], axis=0)\n",
    "            \n",
    "            # Apply color transfer\n",
    "            for c in range(3):\n",
    "                channel = lab[:,:,c]\n",
    "                normalized[:,:,c][mask] = ((channel[mask] - np.mean(channel[mask])) / \n",
    "                                         (np.std(channel[mask]) + 1e-6) * color_std[c] + \n",
    "                                         color_mean[c])\n",
    "        \n",
    "        # Convert back to RGB\n",
    "        return cv2.cvtColor(normalized.astype(np.uint8), cv2.COLOR_LAB2RGB)\n",
    "    \n",
    "    def enhance_contrast_multiscale(self, image):\n",
    "        \"\"\"\n",
    "        Multi-scale contrast enhancement using pyramid decomposition\n",
    "        \"\"\"\n",
    "        # Convert to LAB\n",
    "        lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n",
    "        l, a, b = cv2.split(lab)\n",
    "        \n",
    "        # Build Gaussian pyramid\n",
    "        pyramids = []\n",
    "        current = l.copy()\n",
    "        for _ in range(3):\n",
    "            pyramids.append(current)\n",
    "            current = cv2.pyrDown(current)\n",
    "        \n",
    "        # Enhance each level\n",
    "        enhanced_pyramids = []\n",
    "        for level in pyramids:\n",
    "            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "            enhanced = clahe.apply(np.uint8(level * 255)) / 255.0\n",
    "            enhanced_pyramids.append(enhanced)\n",
    "        \n",
    "        # Reconstruct image\n",
    "        enhanced_l = enhanced_pyramids[0]\n",
    "        for i in range(1, len(enhanced_pyramids)):\n",
    "            enhanced = cv2.resize(enhanced_pyramids[i], \n",
    "                                (enhanced_l.shape[1], enhanced_l.shape[0]))\n",
    "            enhanced_l = cv2.addWeighted(enhanced_l, 0.7, enhanced, 0.3, 0)\n",
    "        \n",
    "        # Merge channels\n",
    "        enhanced = cv2.merge([enhanced_l, a, b])\n",
    "        return cv2.cvtColor(enhanced, cv2.COLOR_LAB2RGB)\n",
    "    \n",
    "    def extract_features(self, image):\n",
    "        \"\"\"\n",
    "        Extract enhanced feature set using advanced CV techniques\n",
    "        \"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # Basic features\n",
    "        features['shape'] = self.extract_shape_features(image)\n",
    "        features['color'] = self.extract_color_features(image)\n",
    "        features['texture'] = self.extract_texture_features(image)\n",
    "        features['border'] = self.extract_border_features(image)\n",
    "        \n",
    "        # Advanced features\n",
    "        features['hog'] = self.extract_hog_features(image)\n",
    "        features['gabor'] = self.extract_gabor_features(image)\n",
    "        features['fractal'] = self.extract_fractal_features(image)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def extract_hog_features(self, image):\n",
    "        \"\"\"\n",
    "        Extract Histogram of Oriented Gradients features\n",
    "        \"\"\"\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # Calculate HOG features\n",
    "        features, hog_image = hog(gray, orientations=9, pixels_per_cell=(8, 8),\n",
    "                                cells_per_block=(2, 2), visualize=True)\n",
    "        \n",
    "        return {\n",
    "            'hog_mean': np.mean(features),\n",
    "            'hog_std': np.std(features),\n",
    "            'hog_max': np.max(features)\n",
    "        }\n",
    "    \n",
    "    def extract_gabor_features(self, image):\n",
    "        \"\"\"\n",
    "        Extract Gabor filter features\n",
    "        \"\"\"\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        features = {}\n",
    "        for theta in range(4):  # 4 orientations\n",
    "            for sigma in [1, 2]:  # 2 scales\n",
    "                filt_real, filt_imag = gabor(gray, frequency=0.6,\n",
    "                                           theta=theta * np.pi / 4,\n",
    "                                           sigma_x=sigma, sigma_y=sigma)\n",
    "                \n",
    "                features[f'gabor_mean_{theta}_{sigma}'] = np.mean(filt_real)\n",
    "                features[f'gabor_var_{theta}_{sigma}'] = np.var(filt_real)\n",
    "                \n",
    "        return features\n",
    "    \n",
    "    def extract_fractal_features(self, image):\n",
    "        \"\"\"\n",
    "        Extract fractal dimension features using box-counting\n",
    "        \"\"\"\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # Calculate edges using Canny\n",
    "        edges = cv2.Canny(np.uint8(gray * 255), 100, 200)\n",
    "        \n",
    "        # Box counting\n",
    "        box_sizes = np.array([2, 3, 4, 6, 8, 12, 16, 32, 64])\n",
    "        counts = []\n",
    "        \n",
    "        for size in box_sizes:\n",
    "            count = np.sum(measure.block_reduce(edges, (size, size),\n",
    "                                              func=np.max) > 0)\n",
    "            counts.append(count)\n",
    "            \n",
    "        coeffs = np.polyfit(np.log(box_sizes), np.log(counts), 1)\n",
    "        \n",
    "        return {\n",
    "            'fractal_dimension': -coeffs[0],\n",
    "            'fractal_intercept': coeffs[1]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Previous code remains the same until the visualization methods]\n",
    "\n",
    "def visualize_analysis(self):\n",
    "    \"\"\"\n",
    "    Enhanced visualization of the analysis results with multiple plots\n",
    "    \"\"\"\n",
    "    # Create a larger figure with subplots\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    gs = fig.add_gridspec(3, 3)\n",
    "    \n",
    "    # Original and basic processing\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax1.imshow(self.results['original'])\n",
    "    ax1.set_title('Original Image')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    ax2.imshow(self.results['mask'], cmap='gray')\n",
    "    ax2.set_title('Lesion Mask')\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    ax3.imshow(self.results['preprocessed'])\n",
    "    ax3.set_title('Preprocessed Image')\n",
    "    ax3.axis('off')\n",
    "    \n",
    "    # Advanced visualizations\n",
    "    if 'mask' in self.results:\n",
    "        # Border detection\n",
    "        ax4 = fig.add_subplot(gs[1, 0])\n",
    "        contours, _ = cv2.findContours(self.results['mask'].astype(np.uint8), \n",
    "                                       cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        border_img = self.results['original'].copy()\n",
    "        cv2.drawContours(border_img, contours, -1, (0, 1, 0), 2)\n",
    "        ax4.imshow(border_img)\n",
    "        ax4.set_title('Border Detection')\n",
    "        ax4.axis('off')\n",
    "        \n",
    "        # HOG visualization\n",
    "        ax5 = fig.add_subplot(gs[1, 1])\n",
    "        gray = cv2.cvtColor(self.results['preprocessed'], cv2.COLOR_RGB2GRAY)\n",
    "        features, hog_image = hog(gray, orientations=9, pixels_per_cell=(8, 8),\n",
    "                                  cells_per_block=(2, 2), visualize=True)\n",
    "        ax5.imshow(hog_image, cmap='gray')\n",
    "        ax5.set_title('HOG Features')\n",
    "        ax5.axis('off')\n",
    "        \n",
    "        # Gabor filter visualization\n",
    "        ax6 = fig.add_subplot(gs[1, 2])\n",
    "        filt_real, _ = gabor(gray, frequency=0.6, theta=0, sigma_x=1, sigma_y=1)\n",
    "        ax6.imshow(filt_real, cmap='gray')\n",
    "        ax6.set_title('Gabor Filter Response')\n",
    "        ax6.axis('off')\n",
    "        \n",
    "        # Multi-scale visualization\n",
    "        ax7 = fig.add_subplot(gs[2, 0])\n",
    "        gradient = filters.sobel(gray)\n",
    "        ax7.imshow(gradient, cmap='gray')\n",
    "        ax7.set_title('Gradient Magnitude')\n",
    "        ax7.axis('off')\n",
    "        \n",
    "        # Color clustering visualization\n",
    "        ax8 = fig.add_subplot(gs[2, 1])\n",
    "        lab = cv2.cvtColor(self.results['preprocessed'], cv2.COLOR_RGB2LAB)\n",
    "        pixels = lab.reshape(-1, 3)\n",
    "        kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "        labels = kmeans.fit_predict(pixels)\n",
    "        segmented = kmeans.cluster_centers_[labels].reshape(lab.shape)\n",
    "        segmented = cv2.cvtColor(segmented.astype(np.uint8), cv2.COLOR_LAB2RGB)\n",
    "        ax8.imshow(segmented)\n",
    "        ax8.set_title('Color Clusters')\n",
    "        ax8.axis('off')\n",
    "        \n",
    "        # Fractal visualization\n",
    "        ax9 = fig.add_subplot(gs[2, 2])\n",
    "        edges = cv2.Canny(np.uint8(gray * 255), 100, 200)\n",
    "        ax9.imshow(edges, cmap='gray')\n",
    "        ax9.set_title('Edge Detection (Fractal Analysis)')\n",
    "        ax9.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_feature_distributions(self, features):\n",
    "    \"\"\"\n",
    "    Plot distributions of extracted features\n",
    "    \"\"\"\n",
    "    # Create color distribution plot\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # RGB distribution\n",
    "    plt.subplot(131)\n",
    "    for i, channel in enumerate(['rgb_1_mean', 'rgb_2_mean', 'rgb_3_mean']):\n",
    "        if channel in features['color']:\n",
    "            plt.bar(i, features['color'][channel], color=['red', 'green', 'blue'][i])\n",
    "    plt.title('RGB Channel Distribution')\n",
    "    plt.xticks(range(3), ['R', 'G', 'B'])\n",
    "    \n",
    "    # Texture features\n",
    "    plt.subplot(132)\n",
    "    texture_features = [f for f in features['texture'].keys() if f.startswith('glcm')]\n",
    "    plt.bar(range(len(texture_features)), [features['texture'][f] for f in texture_features])\n",
    "    plt.title('GLCM Texture Features')\n",
    "    plt.xticks(range(len(texture_features)), texture_features, rotation=45)\n",
    "    \n",
    "    # Shape features\n",
    "    plt.subplot(133)\n",
    "    shape_features = features['shape'].keys()\n",
    "    plt.bar(range(len(shape_features)), [features['shape'][f] for f in shape_features])\n",
    "    plt.title('Shape Features')\n",
    "    plt.xticks(range(len(shape_features)), shape_features, rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def generate_report(self, features):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive analysis report\n",
    "    \"\"\"\n",
    "    print(\"=== SKIN LESION ANALYSIS REPORT ===\\n\")\n",
    "    \n",
    "    # Shape analysis\n",
    "    print(\"SHAPE ANALYSIS:\")\n",
    "    print(f\"Circularity: {features['shape']['circularity']:.3f}\")\n",
    "    print(f\"Asymmetry: {1 - features['shape']['solidity']:.3f}\")\n",
    "    print(f\"Border Irregularity: {1 - features['shape']['circularity']:.3f}\\n\")\n",
    "    \n",
    "    # Color analysis\n",
    "    print(\"COLOR ANALYSIS:\")\n",
    "    color_variation = np.std([features['color'][f] for f in features['color'] if f.startswith('rgb')])\n",
    "    print(f\"Color Variation: {color_variation:.3f}\")\n",
    "    \n",
    "    # Texture analysis\n",
    "    print(\"\\nTEXTURE ANALYSIS:\")\n",
    "    print(f\"Homogeneity: {features['texture']['glcm_homogeneity']:.3f}\")\n",
    "    print(f\"Contrast: {features['texture']['glcm_contrast']:.3f}\")\n",
    "    \n",
    "    # Advanced features\n",
    "    print(\"\\nADVANCED FEATURES:\")\n",
    "    print(f\"Fractal Dimension: {features['fractal']['fractal_dimension']:.3f}\")\n",
    "    print(f\"HOG Feature Mean: {features['hog']['hog_mean']:.3f}\")\n",
    "    \n",
    "    # Border features\n",
    "    print(\"\\nBORDER FEATURES:\")\n",
    "    print(f\"Border Smoothness: {features['border']['smoothness']:.3f}\")\n",
    "    print(f\"Gradient Magnitude: {features['border']['gradient_mean']:.3f}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Example usage of the enhanced skin lesion analyzer\n",
    "    \"\"\"\n",
    "    # Initialize analyzer\n",
    "    analyzer = SkinLesionAnalyzer()\n",
    "    \n",
    "    # Example usage with sample image\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    image_path = 'skin_lesion.jpg'\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Could not load image from {image_path}\")\n",
    "        return\n",
    "        \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Process image\n",
    "    print(\"Processing image...\")\n",
    "    preprocessed = analyzer.preprocess_image(image)\n",
    "    \n",
    "    # Extract features\n",
    "    print(\"Extracting features...\")\n",
    "    features = analyzer.extract_features(preprocessed)\n",
    "    \n",
    "    # Visualize results\n",
    "    print(\"Generating visualizations...\")\n",
    "    analyzer.visualize_analysis()\n",
    "    analyzer.plot_feature_distributions(features)\n",
    "    \n",
    "    # Generate report\n",
    "    print(\"\\nGenerating analysis report...\")\n",
    "    analyzer.generate_report(features)\n",
    "    \"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SkinLesionAnalyzer' object has no attribute 'process_image_folder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 195\u001b[0m\n\u001b[1;32m    192\u001b[0m         analyzer\u001b[38;5;241m.\u001b[39mgenerate_report(features)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 195\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 169\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    166\u001b[0m visualization_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manalysis_visualizations\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# Process images and save results\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m results_df \u001b[38;5;241m=\u001b[39m \u001b[43manalyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_image_folder\u001b[49m(\n\u001b[1;32m    170\u001b[0m     folder_path,\n\u001b[1;32m    171\u001b[0m     save_visualizations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    172\u001b[0m     visualization_folder\u001b[38;5;241m=\u001b[39mvisualization_folder\n\u001b[1;32m    173\u001b[0m )\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# Save results to CSV\u001b[39;00m\n\u001b[1;32m    176\u001b[0m results_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskin_lesion_features.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SkinLesionAnalyzer' object has no attribute 'process_image_folder'"
     ]
    }
   ],
   "source": [
    "def analyze_lesion(self, image):\n",
    "    \"\"\"\n",
    "    Complete analysis pipeline for a single image\n",
    "    \"\"\"\n",
    "    # Preprocess\n",
    "    preprocessed = self.preprocess_image(image)\n",
    "    \n",
    "    # Extract all features\n",
    "    features = self.extract_features(preprocessed)\n",
    "    \n",
    "    # Flatten the nested dictionary of features\n",
    "    flattened_features = {}\n",
    "    for category, feature_dict in features.items():\n",
    "        if feature_dict is not None:  # Check for None values\n",
    "            for feature_name, value in feature_dict.items():\n",
    "                flattened_features[f\"{category}_{feature_name}\"] = value\n",
    "                \n",
    "    return flattened_features\n",
    "\n",
    "def process_image_folder(self, folder_path, save_visualizations=False, visualization_folder=None):\n",
    "    \"\"\"\n",
    "    Process all images in a folder and compile results\n",
    "    \n",
    "    Parameters:\n",
    "    folder_path (str): Path to folder containing images\n",
    "    save_visualizations (bool): Whether to save visualization plots\n",
    "    visualization_folder (str): Path to save visualizations (if save_visualizations is True)\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame containing features for all images\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create visualization folder if needed\n",
    "    if save_visualizations and visualization_folder:\n",
    "        os.makedirs(visualization_folder, exist_ok=True)\n",
    "    \n",
    "    all_features = []\n",
    "    image_names = []\n",
    "    failed_images = []\n",
    "    \n",
    "    # Get all image files\n",
    "    image_files = [f for f in os.listdir(folder_path) \n",
    "                   if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    \n",
    "    # Process each image\n",
    "    for image_file in tqdm(image_files, desc=\"Processing images\"):\n",
    "        try:\n",
    "            # Load and process image\n",
    "            image_path = os.path.join(folder_path, image_file)\n",
    "            img = cv2.imread(image_path)\n",
    "            \n",
    "            if img is None:\n",
    "                raise ValueError(f\"Could not load image: {image_file}\")\n",
    "                \n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Analyze image\n",
    "            features = self.analyze_lesion(img)\n",
    "            \n",
    "            # Save visualizations if requested\n",
    "            if save_visualizations and visualization_folder:\n",
    "                self.save_analysis_visualizations(\n",
    "                    os.path.join(visualization_folder, f\"{os.path.splitext(image_file)[0]}_analysis.png\")\n",
    "                )\n",
    "            \n",
    "            all_features.append(features)\n",
    "            image_names.append(image_file)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing {image_file}: {str(e)}\")\n",
    "            failed_images.append((image_file, str(e)))\n",
    "            continue\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(all_features)\n",
    "    df['image_name'] = image_names\n",
    "    \n",
    "    # Generate batch analysis report\n",
    "    self.generate_batch_analysis_report(df, failed_images)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def save_analysis_visualizations(self, output_path):\n",
    "    \"\"\"\n",
    "    Save current analysis visualizations to file\n",
    "    \"\"\"\n",
    "    # Create figure\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # Add visualizations (same as visualize_analysis method)\n",
    "    self.visualize_analysis()\n",
    "    \n",
    "    # Save to file\n",
    "    plt.savefig(output_path, bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def generate_batch_analysis_report(self, df, failed_images):\n",
    "    \"\"\"\n",
    "    Generate comprehensive report for batch processing results\n",
    "    \"\"\"\n",
    "    print(\"\\n=== BATCH PROCESSING REPORT ===\")\n",
    "    print(f\"\\nTotal images processed: {len(df)}\")\n",
    "    print(f\"Number of features extracted: {len(df.columns) - 1}\")  # -1 for image_name\n",
    "    \n",
    "    if failed_images:\n",
    "        print(\"\\nFailed images:\")\n",
    "        for img, error in failed_images:\n",
    "            print(f\"- {img}: {error}\")\n",
    "    \n",
    "    print(\"\\nFeature Statistics:\")\n",
    "    # Remove image_name column for statistics\n",
    "    feature_df = df.drop('image_name', axis=1)\n",
    "    \n",
    "    # Calculate basic statistics\n",
    "    stats = feature_df.describe()\n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    print(stats)\n",
    "    \n",
    "    # Save statistics to file\n",
    "    stats.to_csv(\"batch_processing_statistics.csv\")\n",
    "    \n",
    "    # Generate feature distribution plots\n",
    "    self.plot_batch_feature_distributions(feature_df)\n",
    "\n",
    "def plot_batch_feature_distributions(self, feature_df):\n",
    "    \"\"\"\n",
    "    Plot distribution of features across all processed images\n",
    "    \"\"\"\n",
    "    # Create directory for plots\n",
    "    os.makedirs(\"feature_distributions\", exist_ok=True)\n",
    "    \n",
    "    # Plot distributions for different feature categories\n",
    "    feature_categories = {\n",
    "        'shape': [col for col in feature_df.columns if col.startswith('shape_')],\n",
    "        'color': [col for col in feature_df.columns if col.startswith('color_')],\n",
    "        'texture': [col for col in feature_df.columns if col.startswith('texture_')],\n",
    "        'border': [col for col in feature_df.columns if col.startswith('border_')],\n",
    "        'advanced': [col for col in feature_df.columns if any(x in col for x in ['hog_', 'gabor_', 'fractal_'])]\n",
    "    }\n",
    "    \n",
    "    for category, features in feature_categories.items():\n",
    "        if features:\n",
    "            plt.figure(figsize=(15, 5))\n",
    "            \n",
    "            # Create box plots for features in this category\n",
    "            feature_df[features].boxplot()\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.title(f'{category.capitalize()} Feature Distributions')\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save plot\n",
    "            plt.savefig(f\"feature_distributions/{category}_distributions.png\")\n",
    "            plt.close()\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Example usage of the enhanced skin lesion analyzer with batch processing\n",
    "    \"\"\"\n",
    "    # Initialize analyzer\n",
    "    analyzer = SkinLesionAnalyzer()\n",
    "    \n",
    "    # Example usage for batch processing\n",
    "    \n",
    "    # Process folder of images\n",
    "    folder_path = \"ISIC-images\"\n",
    "    visualization_folder = \"analysis_visualizations\"\n",
    "    \n",
    "    # Process images and save results\n",
    "    results_df = analyzer.process_image_folder(\n",
    "        folder_path,\n",
    "        save_visualizations=True,\n",
    "        visualization_folder=visualization_folder\n",
    "    )\n",
    "    \n",
    "    # Save results to CSV\n",
    "    results_df.to_csv(\"skin_lesion_features.csv\", index=False)\n",
    "    print(f\"\\nResults saved to 'skin_lesion_features.csv'\")\n",
    "    \n",
    "    # Example of loading and analyzing a single image\n",
    "    image_path = 'sample_lesion.jpg'\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is not None:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Process single image\n",
    "        preprocessed = analyzer.preprocess_image(image)\n",
    "        features = analyzer.extract_features(preprocessed)\n",
    "        \n",
    "        # Visualize results\n",
    "        analyzer.visualize_analysis()\n",
    "        analyzer.plot_feature_distributions(features)\n",
    "        analyzer.generate_report(features)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
